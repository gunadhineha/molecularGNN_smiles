{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gunadhineha/molecularGNN_smiles/blob/master/Machine_Translation_Student_Notebook_New.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KO3M7xEZPZv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "u_-ECG_Yt-ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --folder https://drive.google.com/drive/folders/1VcPv6Pi-P9a0fXek9g6vwru2TFy1yPkp?usp=sharing -O ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0inH2t9IK7I",
        "outputId": "4fd8bf0a-b843-4e11-c895-ddef4e49b912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder list\n",
            "Processing file 1oxBeBA2os9AbRaFKu1cnHx3jN0bcQn0X eng_cleaned.npy\n",
            "Processing file 1yVsXqSzymhHultyVJ-nh5pmQk8c-Py8N jpn_cleaned.npy\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oxBeBA2os9AbRaFKu1cnHx3jN0bcQn0X\n",
            "To: /content/eng_cleaned.npy\n",
            "100% 6.62M/6.62M [00:00<00:00, 99.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1yVsXqSzymhHultyVJ-nh5pmQk8c-Py8N\n",
            "To: /content/jpn_cleaned.npy\n",
            "100% 10.4M/10.4M [00:00<00:00, 88.4MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jpn_dataset = np.load(\"jpn_cleaned.npy\", allow_pickle=True)\n",
        "eng_dataset = np.load(\"eng_cleaned.npy\", allow_pickle=True)"
      ],
      "metadata": {
        "id": "HZgagHUFaK-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jpn_vocab = build_vocab_from_iterator(jpn_dataset, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "eng_vocab = build_vocab_from_iterator(eng_dataset, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "jpn_vocab.set_default_index(jpn_vocab[\"<unk>\"])\n",
        "eng_vocab.set_default_index(eng_vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "n7a62hRsdOTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [(\n",
        "    torch.tensor(jpn_vocab(jpn_text), dtype=torch.long),\n",
        "    torch.tensor(eng_vocab(eng_text), dtype=torch.long)) for (jpn_text, eng_text) in zip(jpn_dataset, eng_dataset)]"
      ],
      "metadata": {
        "id": "k9VRV-QPr_7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "J_PAD_IDX = jpn_vocab['<pad>']\n",
        "J_BOS_IDX = jpn_vocab['<bos>']\n",
        "J_EOS_IDX = jpn_vocab['<eos>']\n",
        "E_PAD_IDX = eng_vocab['<pad>']\n",
        "E_BOS_IDX = eng_vocab['<bos>']\n",
        "E_EOS_IDX = eng_vocab['<eos>']\n",
        "\n",
        "def generate_batch(batch):\n",
        "    jpn_T, eng_T = 0, 0\n",
        "    jpn_list, eng_list = [], []\n",
        "    for (jpn_batch, eng_batch) in batch:\n",
        "        jpn_batch = torch.cat([torch.tensor([J_BOS_IDX]), jpn_batch, torch.tensor([J_EOS_IDX])], dim=0)\n",
        "        eng_batch = torch.cat([torch.tensor([E_BOS_IDX]), eng_batch, torch.tensor([E_EOS_IDX])], dim=0)\n",
        "        jpn_list.append(jpn_batch)\n",
        "        eng_list.append(eng_batch)\n",
        "        jpn_T = max(jpn_T, len(jpn_batch))\n",
        "        eng_T = max(eng_T, len(eng_batch))\n",
        "    if jpn_T > eng_T: eng_list[0] = F.pad(eng_list[0], (0, jpn_T-len(eng_list[0])), value=E_PAD_IDX)\n",
        "    else: jpn_list[0] = F.pad(jpn_list[0], (0, eng_T-len(jpn_list[0])), value=J_PAD_IDX)\n",
        "    jpn_list = pad_sequence(jpn_list, padding_value=J_PAD_IDX).transpose(0,1)\n",
        "    eng_list = pad_sequence(eng_list, padding_value=E_PAD_IDX).transpose(0,1)\n",
        "    return jpn_list.to(device), eng_list.to(device)"
      ],
      "metadata": {
        "id": "mlZlrIWGfsdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=generate_batch)"
      ],
      "metadata": {
        "id": "3bLJhXKpg2g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimplifiedAttention(nn.Module):\n",
        "    def __init__(self, D):\n",
        "      super(SimplifiedAttention, self).__init__()\n",
        "      self.D = D\n",
        "      self.q = nn.Linear(D, D)\n",
        "      self.k = nn.Linear(D, D)\n",
        "      self.v = nn.Linear(D, D)\n",
        "\n",
        "    def forward(self, X):\n",
        "      Q = self.q(X)\n",
        "      K = self.k(X)\n",
        "      V = self.v(X)\n",
        "      S = Q @ torch.transpose(K, 1, 2) / (self.D ** 0.5)\n",
        "      A = F.softmax(S, dim=-1)\n",
        "      Y = A @ V\n",
        "      return Y\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, D):\n",
        "        super(TransformerLayer, self).__init__()\n",
        "        self.sa = SimplifiedAttention(D)\n",
        "        self.ln1 = nn.LayerNorm(D)\n",
        "        self.linear1 = nn.Linear(D, 2*D)\n",
        "        self.linear2 = nn.Linear(2*D, D)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.ln2 = nn.LayerNorm(D)\n",
        "\n",
        "    def forward(self, X):\n",
        "        h = self.sa(X) + X\n",
        "        h = self.ln1(h)\n",
        "        h = self.linear2(self.relu(self.linear1(h))) + h\n",
        "        output = self.ln2(h)\n",
        "        return output\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=500):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_embeddings = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pos_embeddings[:,:x.shape[1]]\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(Model, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = PositionalEncoding(embed_dim)\n",
        "        self.trans1 = TransformerLayer(embed_dim)\n",
        "        self.trans2 = TransformerLayer(embed_dim)\n",
        "        self.trans3 = TransformerLayer(embed_dim)\n",
        "        self.trans4 = TransformerLayer(embed_dim)\n",
        "        self.trans5 = TransformerLayer(embed_dim)\n",
        "        self.trans6 = TransformerLayer(embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "\n",
        "    def forward(self, input):\n",
        "        h = self.embedding(input)\n",
        "        h = self.pos_embedding(h)\n",
        "        h = self.trans1(h)\n",
        "        h = self.trans2(h)\n",
        "        h = self.trans3(h)\n",
        "        h = self.trans4(h)\n",
        "        h = self.trans5(h)\n",
        "        h = self.trans6(h)\n",
        "        output = self.fc(h)\n",
        "        return output"
      ],
      "metadata": {
        "id": "a7Po9s8AeBSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, (src, trg) in enumerate(loader):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        output = model(src)\n",
        "        loss = criterion(output.reshape(-1, output.shape[-1]), trg.reshape(-1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        if i == 0:\n",
        "            jpn_sentence = jpn_vocab.lookup_tokens(src[0].cpu().numpy())\n",
        "            eng_sentence = eng_vocab.lookup_tokens(trg[0].cpu().numpy())\n",
        "            trn_sentence = eng_vocab.lookup_tokens(output[0].argmax(dim=-1).squeeze().cpu().numpy())\n",
        "            jpn_sentence = ''.join(jpn_sentence[1:jpn_sentence.index('<eos>')])\n",
        "            eng_sentence = ' '.join(eng_sentence[1:eng_sentence.index('<eos>')])\n",
        "            if '<eos>' not in trn_sentence: trn_sentence.append('<eos>')\n",
        "            trn_sentence = ' '.join(trn_sentence[1:trn_sentence.index('<eos>')])\n",
        "            print(\"Original sentence: \\t {}\".format(jpn_sentence))\n",
        "            print(\"Target sentence: \\t {}\".format(eng_sentence))\n",
        "            print(\"Model's sentence: \\t {}\".format(trn_sentence))\n",
        "\n",
        "    return epoch_loss / len(loader)"
      ],
      "metadata": {
        "id": "OGuTlVQUigPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    sentence = torch.tensor(jpn_vocab(sentence), dtype=torch.long)\n",
        "    padding = torch.tensor([J_PAD_IDX]*16)\n",
        "    sentence = torch.cat([torch.tensor([J_BOS_IDX]), sentence, torch.tensor([J_EOS_IDX]), padding], dim=0).unsqueeze(0)\n",
        "    sentence = sentence.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(sentence)\n",
        "        predicted_word_idxs = output.argmax(dim=-1).squeeze().cpu().numpy()\n",
        "        translation = eng_vocab.lookup_tokens(predicted_word_idxs)\n",
        "        if '<eos>' not in translation: translation.append('<eos>')\n",
        "        translation = ' '.join(translation[1:translation.index('<eos>')])\n",
        "        print(translation)"
      ],
      "metadata": {
        "id": "7V66AeHKNIv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_vocab_size = len(jpn_vocab)\n",
        "num_class = len(eng_vocab)\n",
        "\n",
        "model = Model(input_vocab_size, 128, num_class).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=E_PAD_IDX)"
      ],
      "metadata": {
        "id": "CAoa-h17jDXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(50):\n",
        "    train_loss = train(model, trainloader, optimizer, criterion)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')"
      ],
      "metadata": {
        "id": "xZ9vsaxni2Yy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93bb3699-fead-4129-925a-82b58df6d694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: \t トムとよく口論になるの？\n",
            "Target sentence: \t do you and tom argue often ?\n",
            "Model's sentence: \t do you and tom often ? ?\n",
            "Epoch: 01\n",
            "\tTrain Loss: 0.707 | Train PPL:   2.028\n",
            "Original sentence: \t 心から尊敬してます。\n",
            "Target sentence: \t i really respect you .\n",
            "Model's sentence: \t i really respect you .\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.701 | Train PPL:   2.015\n",
            "Original sentence: \t 彼は医者になることを望んでいる。\n",
            "Target sentence: \t he wishes to become a doctor .\n",
            "Model's sentence: \t he wishes to a a . .\n",
            "Epoch: 03\n",
            "\tTrain Loss: 0.700 | Train PPL:   2.013\n",
            "Original sentence: \t 誰かここに日本語の話せる人はいませんか。\n",
            "Target sentence: \t does anyone here speak japanese ?\n",
            "Model's sentence: \t is anybody here speak japanese ? ?\n",
            "Epoch: 04\n",
            "\tTrain Loss: 0.695 | Train PPL:   2.005\n",
            "Original sentence: \t 私に怒らないで。\n",
            "Target sentence: \t do n't be mad at me .\n",
            "Model's sentence: \t do n't be mad at me .\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.691 | Train PPL:   1.996\n",
            "Original sentence: \t 明日の朝、電話するよ。\n",
            "Target sentence: \t i 'll call you up tomorrow morning .\n",
            "Model's sentence: \t i 'll give you a ring tomorrow .\n",
            "Epoch: 06\n",
            "\tTrain Loss: 0.692 | Train PPL:   1.998\n",
            "Original sentence: \t 彼は砂糖なしのコーヒーが好みだ。\n",
            "Target sentence: \t he likes coffee without sugar .\n",
            "Model's sentence: \t he likes coffee without sugar .\n",
            "Epoch: 07\n",
            "\tTrain Loss: 0.684 | Train PPL:   1.982\n",
            "Original sentence: \t 私は体の芯まで冷え切っていた。\n",
            "Target sentence: \t i was chilled to the bone .\n",
            "Model's sentence: \t i was chilled to the bone .\n",
            "Epoch: 08\n",
            "\tTrain Loss: 0.685 | Train PPL:   1.983\n",
            "Original sentence: \t 彼は仕事に精通していた。\n",
            "Target sentence: \t he acquainted himself with his job .\n",
            "Model's sentence: \t he acquainted himself with his work job .\n",
            "Epoch: 09\n",
            "\tTrain Loss: 0.683 | Train PPL:   1.980\n",
            "Original sentence: \t 長い長い間私は待たされた。\n",
            "Target sentence: \t i was kept waiting for an eternity .\n",
            "Model's sentence: \t i was kept waiting for wait eternity . .\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.676 | Train PPL:   1.967\n",
            "Original sentence: \t ボランティア活動してるの？\n",
            "Target sentence: \t do you do any volunteer work ?\n",
            "Model's sentence: \t do you volunteering any work work ?\n",
            "Epoch: 11\n",
            "\tTrain Loss: 0.675 | Train PPL:   1.964\n",
            "Original sentence: \t トムが怒ってるって知ってる。\n",
            "Target sentence: \t i know tom is mad .\n",
            "Model's sentence: \t i know tom is is .\n",
            "Epoch: 12\n",
            "\tTrain Loss: 0.672 | Train PPL:   1.958\n",
            "Original sentence: \t トムに何をあげるつもりなの？\n",
            "Target sentence: \t what are you going to give tom ?\n",
            "Model's sentence: \t what are you going to give tom ?\n",
            "Epoch: 13\n",
            "\tTrain Loss: 0.670 | Train PPL:   1.955\n",
            "Original sentence: \t 人は食べ物なしでは生きていけない。\n",
            "Target sentence: \t people ca n't live without food .\n",
            "Model's sentence: \t human people ca ca n't food food .\n",
            "Epoch: 14\n",
            "\tTrain Loss: 0.663 | Train PPL:   1.941\n",
            "Original sentence: \t トムは音楽祭に行った。\n",
            "Target sentence: \t tom went to the music festival .\n",
            "Model's sentence: \t tom went to the music festival\n",
            "Epoch: 15\n",
            "\tTrain Loss: 0.664 | Train PPL:   1.943\n",
            "Original sentence: \t トムはちょうどボストンに戻ったところです。\n",
            "Target sentence: \t tom has just returned to boston .\n",
            "Model's sentence: \t tom has just gone gone boston boston .\n",
            "Epoch: 16\n",
            "\tTrain Loss: 0.662 | Train PPL:   1.939\n",
            "Original sentence: \t お前がいない人生なんて考えられないんだ。\n",
            "Target sentence: \t i ca n't imagine my life without you .\n",
            "Model's sentence: \t i ca n't imagine life without without .\n",
            "Epoch: 17\n",
            "\tTrain Loss: 0.658 | Train PPL:   1.932\n",
            "Original sentence: \t トムは答えを知ってるんだよ。\n",
            "Target sentence: \t tom knows the answer .\n",
            "Model's sentence: \t tom knows the answer .\n",
            "Epoch: 18\n",
            "\tTrain Loss: 0.652 | Train PPL:   1.920\n",
            "Original sentence: \t 二人は来月結婚する事に決めた。\n",
            "Target sentence: \t they decided to get married next month .\n",
            "Model's sentence: \t they decided to to married next month . .\n",
            "Epoch: 19\n",
            "\tTrain Loss: 0.653 | Train PPL:   1.920\n",
            "Original sentence: \t 彼女は危険にさらされている。\n",
            "Target sentence: \t she 's in danger .\n",
            "Model's sentence: \t she is in danger .\n",
            "Epoch: 20\n",
            "\tTrain Loss: 0.649 | Train PPL:   1.914\n",
            "Original sentence: \t 自己紹介をさせてください。\n",
            "Target sentence: \t may i introduce myself ?\n",
            "Model's sentence: \t let me introduce myself .\n",
            "Epoch: 21\n",
            "\tTrain Loss: 0.647 | Train PPL:   1.910\n",
            "Original sentence: \t 歌うつもりですか。\n",
            "Target sentence: \t are you going to sing ?\n",
            "Model's sentence: \t are you going to ?\n",
            "Epoch: 22\n",
            "\tTrain Loss: 0.642 | Train PPL:   1.900\n",
            "Original sentence: \t なんで笑ってるの？\n",
            "Target sentence: \t why are you laughing ?\n",
            "Model's sentence: \t why are you laughing at ?\n",
            "Epoch: 23\n",
            "\tTrain Loss: 0.645 | Train PPL:   1.906\n",
            "Original sentence: \t トムは2時30分に電話すると言った。\n",
            "Target sentence: \t tom said he 'd phone at half past two .\n",
            "Model's sentence: \t tom said he 'd call at at 2:30\n",
            "Epoch: 24\n",
            "\tTrain Loss: 0.637 | Train PPL:   1.892\n",
            "Original sentence: \t トムが何かを隠すのが見えた。\n",
            "Target sentence: \t i saw tom hiding something .\n",
            "Model's sentence: \t i saw tom hide something .\n",
            "Epoch: 25\n",
            "\tTrain Loss: 0.638 | Train PPL:   1.892\n",
            "Original sentence: \t チケット代は、お持ちですか？\n",
            "Target sentence: \t do you have money for a ticket ?\n",
            "Model's sentence: \t do you have money for a ticket ?\n",
            "Epoch: 26\n",
            "\tTrain Loss: 0.635 | Train PPL:   1.887\n",
            "Original sentence: \t 彼女は試験に合格した。\n",
            "Target sentence: \t she passed the examination .\n",
            "Model's sentence: \t she passed the examination .\n",
            "Epoch: 27\n",
            "\tTrain Loss: 0.632 | Train PPL:   1.882\n",
            "Original sentence: \t 路上の血痕は俺のものに違いない。\n",
            "Target sentence: \t the blood on the road must be mine .\n",
            "Model's sentence: \t the blood on the road be be .\n",
            "Epoch: 28\n",
            "\tTrain Loss: 0.654 | Train PPL:   1.924\n",
            "Original sentence: \t 適当に入って勝手にやって。\n",
            "Target sentence: \t come on in and make yourself at home .\n",
            "Model's sentence: \t come really and and make yourself at home\n",
            "Epoch: 29\n",
            "\tTrain Loss: 0.626 | Train PPL:   1.870\n",
            "Original sentence: \t 私はこっちの方がいいな。\n",
            "Target sentence: \t i like this one better .\n",
            "Model's sentence: \t i like this one one . .\n",
            "Epoch: 30\n",
            "\tTrain Loss: 0.624 | Train PPL:   1.867\n",
            "Original sentence: \t 詐欺臭いな。\n",
            "Target sentence: \t that sounds like a scam .\n",
            "Model's sentence: \t that sounds like a scam .\n",
            "Epoch: 31\n",
            "\tTrain Loss: 0.626 | Train PPL:   1.869\n",
            "Original sentence: \t ピクニックに出かけよう。\n",
            "Target sentence: \t let 's go to the picnic .\n",
            "Model's sentence: \t let 's go the the picnic\n",
            "Epoch: 32\n",
            "\tTrain Loss: 0.624 | Train PPL:   1.867\n",
            "Original sentence: \t 私の部屋はとても狭いの。\n",
            "Target sentence: \t my room is very small .\n",
            "Model's sentence: \t my room is very very .\n",
            "Epoch: 33\n",
            "\tTrain Loss: 0.622 | Train PPL:   1.863\n",
            "Original sentence: \t トムは私より多くの魚を捕まえた。\n",
            "Target sentence: \t tom caught more fish than me .\n",
            "Model's sentence: \t tom caught more fish than me did .\n",
            "Epoch: 34\n",
            "\tTrain Loss: 0.622 | Train PPL:   1.862\n",
            "Original sentence: \t トムはもう来た？\n",
            "Target sentence: \t has tom arrived yet ?\n",
            "Model's sentence: \t has tom arrived yet ?\n",
            "Epoch: 35\n",
            "\tTrain Loss: 0.616 | Train PPL:   1.852\n",
            "Original sentence: \t 彼は誰かに部屋を掃除させるだろう。\n",
            "Target sentence: \t he 'll make someone clean the room .\n",
            "Model's sentence: \t he 'll make someone clean the .\n",
            "Epoch: 36\n",
            "\tTrain Loss: 0.613 | Train PPL:   1.845\n",
            "Original sentence: \t 趣味についてお聞かせください。\n",
            "Target sentence: \t please tell me about your hobbies .\n",
            "Model's sentence: \t please tell me about your hobbies .\n",
            "Epoch: 37\n",
            "\tTrain Loss: 0.613 | Train PPL:   1.846\n",
            "Original sentence: \t 古い家が取り壊された。\n",
            "Target sentence: \t the old house was demolished .\n",
            "Model's sentence: \t the old house was taken down .\n",
            "Epoch: 38\n",
            "\tTrain Loss: 0.611 | Train PPL:   1.842\n",
            "Original sentence: \t 詐欺臭いな。\n",
            "Target sentence: \t it sounds like a scam .\n",
            "Model's sentence: \t that sounds like a scam .\n",
            "Epoch: 39\n",
            "\tTrain Loss: 0.608 | Train PPL:   1.837\n",
            "Original sentence: \t これは手触りが柔らかくて、なめらかですね。\n",
            "Target sentence: \t this feels soft and smooth .\n",
            "Model's sentence: \t this feels soft and smooth .\n",
            "Epoch: 40\n",
            "\tTrain Loss: 0.623 | Train PPL:   1.864\n",
            "Original sentence: \t 気分はどうですか。\n",
            "Target sentence: \t how do you feel now ?\n",
            "Model's sentence: \t how do you feel ?\n",
            "Epoch: 41\n",
            "\tTrain Loss: 0.605 | Train PPL:   1.832\n",
            "Original sentence: \t ここにはよく来られるんですか？\n",
            "Target sentence: \t do you often come here ?\n",
            "Model's sentence: \t do you come come often ? ?\n",
            "Epoch: 42\n",
            "\tTrain Loss: 0.602 | Train PPL:   1.826\n",
            "Original sentence: \t きっと別のやり方もあるはずだよ。\n",
            "Target sentence: \t there has to be another way to do this .\n",
            "Model's sentence: \t it can to to to way do . .\n",
            "Epoch: 43\n",
            "\tTrain Loss: 0.602 | Train PPL:   1.825\n",
            "Original sentence: \t このテレビはバーゲンで買ったんだ。\n",
            "Target sentence: \t i bought this tv set at a bargain sale .\n",
            "Model's sentence: \t i bought this tv set at a bargain sale sale\n",
            "Epoch: 44\n",
            "\tTrain Loss: 0.601 | Train PPL:   1.824\n",
            "Original sentence: \t そのホテルは見晴らしがよい。\n",
            "Target sentence: \t the view from that hotel is spectacular .\n",
            "Model's sentence: \t the view from n't hotel is .\n",
            "Epoch: 45\n",
            "\tTrain Loss: 0.603 | Train PPL:   1.827\n",
            "Original sentence: \t 客の足が遠のいた。\n",
            "Target sentence: \t customers stopped coming to our shop .\n",
            "Model's sentence: \t customers proposal coming to our shop .\n",
            "Epoch: 46\n",
            "\tTrain Loss: 0.595 | Train PPL:   1.813\n",
            "Original sentence: \t このストーブは灯油を使用します。\n",
            "Target sentence: \t this stove uses kerosene .\n",
            "Model's sentence: \t this stove uses kerosene .\n",
            "Epoch: 47\n",
            "\tTrain Loss: 0.598 | Train PPL:   1.819\n",
            "Original sentence: \t この花瓶は壊れやすい。\n",
            "Target sentence: \t this vase is fragile .\n",
            "Model's sentence: \t this vase is fragile .\n",
            "Epoch: 48\n",
            "\tTrain Loss: 0.603 | Train PPL:   1.828\n",
            "Original sentence: \t とうとうあの本を出版しましたよ。\n",
            "Target sentence: \t we finally published the book .\n",
            "Model's sentence: \t we finally published published book .\n",
            "Epoch: 49\n",
            "\tTrain Loss: 0.589 | Train PPL:   1.802\n",
            "Original sentence: \t 僕たちはみんな、トムが好き。\n",
            "Target sentence: \t we all like tom .\n",
            "Model's sentence: \t all all us tom tom\n",
            "Epoch: 50\n",
            "\tTrain Loss: 0.588 | Train PPL:   1.800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(['私', 'は', 'コーヒー', 'が', '大嫌い', 'です', '。'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10FZuPc2kMvZ",
        "outputId": "d74b9e84-80e7-4a2b-bb02-a340d3cd97b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i hate coffee .\n"
          ]
        }
      ]
    }
  ]
}